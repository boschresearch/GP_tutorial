{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2018 Robert Bosch GmbH\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\n",
    "@author: Barbara Rakitsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import gpflow\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is completely specified by its mean function $m(x)$ and its covariance function $k(x,x′)$:\n",
    "\n",
    "$f(x)∼GP(m(x),k(x,x′))$\n",
    "\n",
    "In the following, we will show how to apply Gaussian Processes to a regression dataset using a zero mean function and a squared exponential kernel. The choice of the mean and the kernel function encodes our prior beliefs of the function.\n",
    "In the training step, we restrict the fucntions of the prior such that they match the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_train = 30 # number of training points\n",
    "N_test = 100 # number of test points\n",
    "num_samples = 1 # number of Monte Carlo samples in prediction\n",
    "variance = 0.1 # variance of the noise\n",
    "\n",
    "# generate synthetic features\n",
    "X_test = np.linspace(0,10,N_test)[:,np.newaxis]\n",
    "idx = np.random.randint(0, N_test, size=N_train)\n",
    "X_train = X_test[idx]\n",
    "\n",
    "# generate synthetic output by drawing a sample from the GP prior\n",
    "kern = gpflow.kernels.SquaredExponential(1)\n",
    "K = kern.compute_K_symm(X_train)\n",
    "zeros = np.zeros(N_train)\n",
    "f_train  = np.random.multivariate_normal(zeros, K, 1).T\n",
    "# add observational noise\n",
    "y_train  = f_train + np.sqrt(variance) * np.random.randn(N_train,1)\n",
    "\n",
    "# initialize gaussian process regression object\n",
    "gpr =  gpflow.models.gpr.GPR(X_train, y_train, kern)\n",
    "\n",
    "# fit the parameters of the kernel and of the variance of the noise to the training data\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(gpr)\n",
    "\n",
    "# predict to un-seen test data\n",
    "Fhat_test = gpr.predict_f_samples(X_test, num_samples)[:,:,0].T # random samples of the GP posterior\n",
    "Fhat_mean, Fhat_var = gpr.predict_f(X_test) # mean and variance of the predictions\n",
    "\n",
    "# plot data\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(X_train, y_train, 'ko', markersize=3)\n",
    "plt.plot(X_test, Fhat_test)\n",
    "plt.fill_between(X_test[:,0], -2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], +2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], color='k', alpha=0.1)\n",
    "utils.beautify_plot(ax, 'X', 'Y')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite Kernels\n",
    "\n",
    "The power of Gaussian Processes lies in choosing an expressive covariance function. In the following, we show how we can build powerful kernels by combining base kernels. \n",
    "\n",
    "In our example, we choose as covariance function a sum of a linear and a squared exponential kernel. The squared exponential kernel belongs to the class of stationary kernel functions - it takes only the relative distance between two data points into account. In case of extrapolation, the GP predictions fall back to the prior.\n",
    "The linear kernel is equivalent to Bayesian linear regression and is a non-strationary kernel. It does not fall back to the prior when it comes to extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 100 # number of training points\n",
    "N_test = 1000 # number of test points\n",
    "num_samples = 1 # number of Monte Carlo samples in prediction\n",
    "variance = 0.05 # variance of the noise\n",
    "\n",
    "# generate synthetic features\n",
    "X_test = np.linspace(0,10,N_test)[:,np.newaxis]\n",
    "idx = np.random.randint(0, N_test/2, size=N_train)\n",
    "X_train = X_test[idx]\n",
    "\n",
    "# generate synthetic output by drawing samples from the base GP priors and adding them up\n",
    "kern_lin = gpflow.kernels.Linear(1)\n",
    "kern_se  = gpflow.kernels.SquaredExponential(1, variance=0.5, lengthscales=0.1)\n",
    "K_lin = kern_lin.compute_K_symm(X_train)\n",
    "K_se  = kern_se.compute_K_symm(X_train)\n",
    "zeros = np.zeros(N_train)\n",
    "f_lin = np.random.multivariate_normal(zeros, K_lin, 1).T\n",
    "f_se  = np.random.multivariate_normal(zeros, K_se, 1).T\n",
    "f_train = f_lin + f_se\n",
    "# add observational noise\n",
    "y_train  = f_train + np.sqrt(variance) * np.random.randn(N_train,1)\n",
    "\n",
    "# initialize gaussian process regression object\n",
    "kern = gpflow.kernels.Sum([kern_lin, kern_se]) # composite kernel\n",
    "gpr =  gpflow.models.gpr.GPR(X_train, y_train, kern)\n",
    "\n",
    "# fit the parameters of the kernel and of the variance of the noise to the training data\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(gpr)\n",
    "\n",
    "# predict to un-seen test data\n",
    "Fhat_test = gpr.predict_f_samples(X_test, num_samples)[:,:,0].T # random samples of the GP posterior\n",
    "Fhat_mean, Fhat_var = gpr.predict_f(X_test) # mean and variance of the predictions\n",
    "\n",
    "# plot data\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(X_train, y_train, 'ko', markersize=3)\n",
    "plt.plot(X_test, Fhat_test)\n",
    "plt.fill_between(X_test[:,0], -2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], +2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], color='k', alpha=0.1)\n",
    "utils.beautify_plot(ax, 'X', 'Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Gaussian Processes\n",
    "\n",
    "Standard Gaussian Processes scale cubically with the number of data points $N$ which can be prohibitive for large-scale data sets. Sparse Gaussian Processes arrive at scalable solutions by capturing the information of the training data set in $M$ inducing points. The number of inducing points $M$ is a trade-off parameter between scalability and the quality of the approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy GP predictions\n",
    "N_train = 100 # number of training points\n",
    "N_test = 100 # number of test points\n",
    "num_samples = 3 # number of Monte Carlo samples\n",
    "variance = 0.1 # variance of the noise\n",
    "\n",
    "# generate synthetic features\n",
    "idx = np.random.randint(0, N_test, size=N_train)\n",
    "X_test = np.linspace(0,10,N_test)[:,np.newaxis]\n",
    "X_train = X_test[idx]\n",
    "\n",
    "# generate synthetic output by drawing a sample from the GP prior \n",
    "kern = gpflow.kernels.SquaredExponential(1, lengthscales=0.4)\n",
    "K = kern.compute_K_symm(X_train)\n",
    "zeros = np.zeros(N_train)\n",
    "f_train  = np.random.multivariate_normal(zeros, K, 1).T\n",
    "# add observational noise\n",
    "y_train  = f_train + np.sqrt(variance) * np.random.randn(N_train,1)\n",
    "\n",
    "# standard Gaussian Process regression\n",
    "gpr =  gpflow.models.gpr.GPR(X_train, y_train, kern)\n",
    "gpr.likelihood.variance = variance\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(gpr)\n",
    "Fhat_test = gpr.predict_f_samples(X_test, num_samples)[:,:,0].T\n",
    "Fhat_mean, Fhat_var = gpr.predict_f(X_test)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "fig.subplots_adjust(bottom=0.2, left=0.2)\n",
    "ax = fig.add_subplot(211)\n",
    "plt.title('GPR')\n",
    "plt.plot(X_train, y_train, 'ko', markersize=3)\n",
    "plt.plot(X_test, Fhat_mean)\n",
    "plt.fill_between(X_test[:,0], -2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], +2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], color='b', alpha=0.1)\n",
    "utils.beautify_plot(ax)\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# sparse Gaussian Process regression\n",
    "M = 15\n",
    "# inducing points are intialized by running K-Means over the training data\n",
    "Z = sklearn.cluster.KMeans(M).fit(X_train).cluster_centers_ \n",
    "# initialize sparse Gaussian Process object\n",
    "gpr = gpflow.models.sgpr.SGPR(X_train, y_train, kern, Z=Z)\n",
    "# fit the parameters of the kernel and of the variance of the noise and the inducing points to the training data\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(gpr)\n",
    "Fhat_test = gpr.predict_f_samples(X_test, num_samples)[:,:,0].T\n",
    "Fhat_mean, Fhat_var = gpr.predict_f(X_test)\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "plt.title('SGPR')\n",
    "plt.plot(X_train, y_train, 'ko', markersize=3)\n",
    "plt.plot(X_test, Fhat_mean)\n",
    "plt.fill_between(X_test[:,0], -2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], +2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], color='b', alpha=0.1)\n",
    "utils.beautify_plot(ax, 'X', 'Y')\n",
    "for i in range(Z.shape[0]):\n",
    "    plt.axvline(gpr.feature.Z.value[i], color='Orange', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Exogenous Gaussian Process regression\n",
    "\n",
    "In the examples above, we used Gaussian Processes to analyse regression datasets. In this last example, we want to forecast\n",
    "time-series data by taking the input history into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSteps = 1000 # number of time steps\n",
    "T = np.linspace(0, 30, nSteps)[:,np.newaxis] # time points \n",
    "variance = 0.05 # variance of the noise\n",
    "\n",
    "# generate synthetic features\n",
    "kern = gpflow.kernels.SquaredExponential(1)\n",
    "K = kern.compute_K_symm(T)\n",
    "zeros = np.zeros(nSteps)\n",
    "X = np.random.multivariate_normal(zeros, K, 1).T\n",
    "\n",
    "# generate features by taking the history of the input into account\n",
    "D = 5 # number of history events\n",
    "delta = 10 # interval between history events\n",
    "X_blowup = np.zeros((nSteps, D))\n",
    "for i in range(D):\n",
    "    if i==0:\n",
    "        X_blowup[:,i] = X[:,0] # original inputs\n",
    "    else:\n",
    "        X_blowup[i*delta:,i] = X[:-(i*delta), 0] # inputs from past time point\n",
    "\n",
    "# generate synthetic output by drawing a sample from the GP prior      \n",
    "kern = gpflow.kernels.SquaredExponential(D)\n",
    "K = kern.compute_K_symm(X_blowup)\n",
    "zeros = np.zeros(nSteps)\n",
    "F = np.random.multivariate_normal(zeros, K, 1).T\n",
    "# add observational noise\n",
    "Y = F + np.sqrt(variance) * np.random.randn(nSteps,1)\n",
    "\n",
    "# splitting data ino training and test samples\n",
    "idx_train = np.ones(nSteps, dtype=np.bool) \n",
    "idx_train[700:] = 0\n",
    "T_train = T[idx_train]\n",
    "T_test = T[~idx_train]\n",
    "X_train = X[idx_train]\n",
    "X_blowup_train = X_blowup[idx_train]\n",
    "Y_train = Y[idx_train]\n",
    "X_test  = X[~idx_train]\n",
    "X_blowup_test = X_blowup[~idx_train]\n",
    "Y_test  = Y[~idx_train]\n",
    "\n",
    "# plot features\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(311)\n",
    "plt.plot(T, X)\n",
    "plt.xticks(())\n",
    "utils.beautify_plot(ax, ylabel='X')\n",
    "\n",
    "# model data not taking any history into account\n",
    "kern = gpflow.kernels.SquaredExponential(1)\n",
    "gpr =  gpflow.models.gpr.GPR(X_train, Y_train, kern)\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(gpr)\n",
    "Fhat_mean, Fhat_var = gpr.predict_y(X)\n",
    "\n",
    "ax = fig.add_subplot(312)\n",
    "plt.title('GP')\n",
    "plt.plot(T, F)\n",
    "plt.plot(T_train, Y_train, '*', alpha=0.1)\n",
    "utils.beautify_plot(ax, ylabel='Y')\n",
    "plt.xticks(())\n",
    "plt.fill_between(T[:,0], -2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], +2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], color='k', alpha=0.1)\n",
    "utils.beautify_plot(ax, ylabel='Y')\n",
    "\n",
    "# model data taking history into account\n",
    "kern = gpflow.kernels.SquaredExponential(D)\n",
    "gpr =  gpflow.models.gpr.GPR(X_blowup_train, Y_train, kern)\n",
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(gpr)\n",
    "Fhat_mean, Fhat_var = gpr.predict_y(X_blowup)\n",
    "\n",
    "ax = fig.add_subplot(313)\n",
    "plt.title('GP-NX')\n",
    "plt.plot(T, F)\n",
    "plt.plot(T_train, Y_train, '*', alpha=0.1)\n",
    "plt.fill_between(T[:,0], -2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], +2*np.sqrt(Fhat_var[:,0]) + Fhat_mean[:,0], color='k', alpha=0.1)\n",
    "utils.beautify_plot(ax, 'T', 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
